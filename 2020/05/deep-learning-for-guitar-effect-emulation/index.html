<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Deep Learning for Guitar Effect Emulation | Teddy Koker</title>
<meta name="generator" content="Jekyll v4.0.1" />
<meta property="og:title" content="Deep Learning for Guitar Effect Emulation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Since the 1940s, electric guitarists, keyboardists, and other instrumentalists have been using effects pedals, devices that modify the sound of the original audio source. Typical effects include distortion, compression, chorus, reverb, and delay. Early effects pedals consisted of basic analog circuits, often along with vacuum tubes, which were later replaced with transistors. Although many pedals today apply effects digitally with modern signal processing techniques, many purists argue that the sound of analog pedals can not be replaced by their digital counterparts. We’ll follow a deep learning approach to see if we can use machine learning to replicate the sound of an iconic analog effect pedal, the Ibanez Tube Screamer. This post will be mostly a reproduction of the work done by Alec Wright et al. in Real-Time Guitar Amplifier Emulation with Deep Learning1. Alec Wright et al., “Real-Time Guitar Amplifier Emulation with &#8617;" />
<meta property="og:description" content="Since the 1940s, electric guitarists, keyboardists, and other instrumentalists have been using effects pedals, devices that modify the sound of the original audio source. Typical effects include distortion, compression, chorus, reverb, and delay. Early effects pedals consisted of basic analog circuits, often along with vacuum tubes, which were later replaced with transistors. Although many pedals today apply effects digitally with modern signal processing techniques, many purists argue that the sound of analog pedals can not be replaced by their digital counterparts. We’ll follow a deep learning approach to see if we can use machine learning to replicate the sound of an iconic analog effect pedal, the Ibanez Tube Screamer. This post will be mostly a reproduction of the work done by Alec Wright et al. in Real-Time Guitar Amplifier Emulation with Deep Learning1. Alec Wright et al., “Real-Time Guitar Amplifier Emulation with &#8617;" />
<link rel="canonical" href="https://teddykoker.com/2020/05/deep-learning-for-guitar-effect-emulation/" />
<meta property="og:url" content="https://teddykoker.com/2020/05/deep-learning-for-guitar-effect-emulation/" />
<meta property="og:site_name" content="Teddy Koker" />
<meta property="og:image" content="https://teddykoker.com/images/signal_chain.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-10T00:00:00-04:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://teddykoker.com/images/signal_chain.png" />
<meta property="twitter:title" content="Deep Learning for Guitar Effect Emulation" />
<meta name="twitter:site" content="@teddykoker" />
<script type="application/ld+json">
{"description":"Since the 1940s, electric guitarists, keyboardists, and other instrumentalists have been using effects pedals, devices that modify the sound of the original audio source. Typical effects include distortion, compression, chorus, reverb, and delay. Early effects pedals consisted of basic analog circuits, often along with vacuum tubes, which were later replaced with transistors. Although many pedals today apply effects digitally with modern signal processing techniques, many purists argue that the sound of analog pedals can not be replaced by their digital counterparts. We’ll follow a deep learning approach to see if we can use machine learning to replicate the sound of an iconic analog effect pedal, the Ibanez Tube Screamer. This post will be mostly a reproduction of the work done by Alec Wright et al. in Real-Time Guitar Amplifier Emulation with Deep Learning1. Alec Wright et al., “Real-Time Guitar Amplifier Emulation with &#8617;","image":"https://teddykoker.com/images/signal_chain.png","@type":"BlogPosting","headline":"Deep Learning for Guitar Effect Emulation","dateModified":"2020-05-10T00:00:00-04:00","datePublished":"2020-05-10T00:00:00-04:00","url":"https://teddykoker.com/2020/05/deep-learning-for-guitar-effect-emulation/","mainEntityOfPage":{"@type":"WebPage","@id":"https://teddykoker.com/2020/05/deep-learning-for-guitar-effect-emulation/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://teddykoker.com/feed.xml" title="Teddy Koker" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-138897125-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
<link rel="shortcut icon" href="/favicon.png">

  <!-- Katex Math (use defer to speed page load) -->
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
        integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq"
        crossorigin="anonymous">

  <script defer
          src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
          integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz"
          crossorigin="anonymous"></script>

  <!--
  <script defer
          src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mathtex-script-type.min.js"
          integrity="sha384-LJ2FmexL77rmGm6SIpxq7y+XA6bkLzGZEgCywzKOZG/ws4va9fUVu2neMjvc3zdv"
          crossorigin="anonymous" ></script> -->

  <script defer
          src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
          integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
          crossorigin="anonymous"
          onload='renderMathInElement(document.body,{delimiters: [{left: "\\[",
          right: "\\]", display: true}, {left: "$", right: "$", display: false}]})'></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Teddy Koker</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/resume.pdf" onclick="ga('send', 'event', 'pdf', 'download', 'resume')">Résumé</a>
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep Learning for Guitar Effect Emulation</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-05-10T00:00:00-04:00" itemprop="datePublished">May 10, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Since the 1940s, electric guitarists, keyboardists, and other
instrumentalists have been using <a href="https://en.wikipedia.org/wiki/Effects_unit">effects
pedals</a>, devices that modify
the sound of the original audio source. Typical effects include
distortion, compression, chorus, reverb, and delay. Early effects pedals
consisted of basic analog circuits, often along with vacuum tubes, which
were later replaced with transistors. Although many pedals today apply
effects digitally with modern signal processing techniques, many purists
argue that the sound of analog pedals can not be replaced by their
digital counterparts. We’ll follow a deep learning approach to see if we
can use machine learning to replicate the sound of an iconic analog
effect pedal, the <a href="https://en.wikipedia.org/wiki/Ibanez_Tube_Screamer">Ibanez Tube
Screamer</a>. This post
will be mostly a reproduction of the work done by Alec Wright et al. in
<em>Real-Time Guitar Amplifier Emulation with Deep Learning</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<!--more-->
<p>The code for this model (and training data) is available here:
<a href="https://github.com/teddykoker/pedalnet">github.com/teddykoker/pedalnet</a>.</p>

<h2 id="data">Data</h2>

<p>Popularized by blues guitarist Stevie Ray Vaughan, the Ibanez Tube
Screamer is used by many well known guitarists including Gary Clark Jr.,
The Edge (U2), Noel Gallagher (Oasis), Billie Joe Armstrong (Green Day),
John Mayer, Eric Johnson, Carlos Santana, and many more<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Using my
own Ibanez TS9 Tube Screamer, we collect data by connecting the pedal to
an audio interface and recording the output of a dataset of prerecorded
guitar playing. The
<a href="https://www.idmt.fraunhofer.de/en/business_units/m2d/smt/guitar.html">IDMT-SMT-Guitar</a>
dataset contains dry signal recordings of many different electric
guitars with both monophonic and polyphonic phrases over different
genres and playing techniques<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>. We’ll use a 5 minute subset of this
data, and store both the original audio as well as the output of the
pedal when the audio is passed through it. To maintain reproducibility,
we set all of the knobs on both the pedal and audio interface to 12
o’clock:</p>

<p><img src="/images/signal_chain.png" height="400" width="auto" style="margin: 0 auto; display: block;" /></p>

<h2 id="model">Model</h2>

<p>Our model architecture will be nearly identical to that of <em>WaveNet: A
Generative Model for Raw Audio</em><sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>. WaveNet models are able to generate
audio that is both qualitatively and quantitatively better than more
traditional LSTM and statistical-based models.</p>

<h3 id="dilated-convolutions">Dilated Convolutions</h3>

<p>The “main ingredient” of the WaveNet architecture consists of a stack of
dilated convolutions, or <em>à trous</em>, layers. By doubling the dilation –
increasing the spacing between each parameter in the filter – for each
layer , the receptive field of the model grows exponentially with depth
of the model. This allows for computationally efficient models with
large receptive fields, which is needed for audio effect emulation.</p>

<p><img src="/images/dilated_conv.png" alt="" /> Figure 3 from WaveNet: Visualization of a
stack of <em>dilated</em> convolutional layers.</p>

<h3 id="gated-activation-units">Gated Activation Units</h3>

<p>Another notable feature of WaveNet architecture is the gated activation
unit. The output of each layer is computed as:</p>

\[z = \tanh \left(W_{f, k} \ast x\right) \odot \sigma \left(W_{g, k} \ast x
\right)\]

<p>where $\ast$, $\odot$, and $\sigma(\cdot)$ denote convolution,
element-wise multiplication, and the sigmoid function, respectively.
$W_{f, k}$ and $W_{g, k}$ are the learned convolutionial filters at
layer $k$. This was found to produce better results than the
traditionally used rectified linear activation unit (ReLU).</p>

<h3 id="differences-from-wavenet">Differences From WaveNet</h3>

<p>The WaveNet model originally quantizes 16-bit audio time samples into
256 bins, and the model is trained to produce a probability distribution
over these 256 possible values. In order to reduce the size of the model
and increase its inference speed, we replace the 256 channel discrete
output with a single continuous output. This is done by performing a
$1 \times 1$ convolution on the concatenation of each layer’s output.</p>

<h2 id="training">Training</h2>

<p>To train our network, we minimize error-to-signal ratio. This is similar
to Mean Squared Error (MSE), however the addition of the term in the
denominator normalizes the loss with respect to the amplitude of the
target signal:</p>

\[L_\text{ESR} = \frac
{\sum_{t} (H(y_t) - H(\hat{y}_t))^2}
{\sum_{t} H(y_t)^2}\]

<p>where $\hat{y}$ is the predicted signal, and $y$ is the original output
of the guitar pedal. $H(\cdot)$ is a pre-emphasis filter to emphasize
frequencies within the audible spectrum:</p>

\[H(z_t) = 1 - 0.95 z_{t-1}\]

<p>When selecting the number of layers and channels for the model, we find
that a a stack of 24 layers, each with 16 channels, and a dilatation
pattern of:</p>

\[1, 2, 4,..., 256, 1, 2, 4,..., 256, 1, 2, 4, ..., 256\]

<p>was capable of replicating the sound well, while being small enough to
run in real time on a CPU. The model is then trained for 1500 epochs
using the Adam optimizer. This takes about 2 hours on a single Nvidia
2070 GPU.</p>

<h2 id="results">Results</h2>

<p>After training our network, we can listen to the models performance on
the held-out test set. See if you can differentiate between <strong>Output A</strong>
and <strong>Output B</strong> (you may need to wear headphones).</p>

<h4 id="input-dry-signal">Input (Dry Signal)</h4>

<audio src="/images/x_test_0.wav" controls="" preload="">
</audio>
<p><br /> <audio src="/images/x_test_1.wav" controls="" preload=""></audio></p>

<h4 id="output-a">Output A</h4>

<audio src="/images/y_pred_0.wav" controls="" preload="">
</audio>
<p><br /> <audio src="/images/y_pred_1.wav" controls="" preload=""></audio></p>

<h4 id="output-b">Output B</h4>

<audio src="/images/y_test_0.wav" controls="" preload="">
</audio>
<p><br /> <audio src="/images/y_test_1.wav" controls="" preload=""></audio></p>

<details>
<summary>Reveal Outputs</summary>
<p>
<br /> <b>Output A</b> is from the neural net; <b>Output B</b> is from the
real pedal.
</p>
</details>
<p><br /></p>

<p>We find that the model is able to reproduce a sound nearly
indistinguishable from the real analog pedal. Best of all, the model is
small and efficient enough to be used in real time. Using this
technique, many analog effect pedals can likely be modeled with just a
few minutes of sample audio.</p>

<p>As always, thank you for reading! For any questions regarding this post
or others, feel free to reach out on twitter:
<a href="https://twitter.com/teddykoker">@teddykoker</a>.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Alec Wright et al., “Real-Time Guitar Amplifier Emulation with
Deep Learning,” <em>Applied Sciences</em> 10, no. 3 (2020): 766. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Wikipedia, <em>Ibanez Tube Screamer</em>, 2020,
<a href="https://en.wikipedia.org/wiki/Ibanez_Tube_Screamer">https://en.wikipedia.org/wiki/Ibanez_Tube_Screamer</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Christian Kehling et al., “Automatic Tablature Transcription of
Electric Guitar Recordings by Estimation of Score-and
Instrument-Related Parameters.” in <em>DAFx</em>, 2014, 219–26. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Aaron van den Oord et al., “Wavenet: A Generative Model for Raw
Audio,” <em>arXiv Preprint arXiv:1609.03499</em>, 2016. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/2020/05/deep-learning-for-guitar-effect-emulation/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Teddy Koker</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Teddy Koker</li><li><a class="u-email" href="mailto:teddy.koker@gmail.com">teddy.koker@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/teddykoker"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">teddykoker</span></a></li><li><a href="https://www.linkedin.com/in/teddykoker"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">teddykoker</span></a></li><li><a href="https://www.twitter.com/teddykoker"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">teddykoker</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Algorithmic Trading and Machine Learning.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
